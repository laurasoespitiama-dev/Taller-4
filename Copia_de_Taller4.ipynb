{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfVWIU+m8OF2/xAG1QpMBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurasoespitiama-dev/Taller-4/blob/main/Copia_de_Taller4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SISTEMAS AVANZADOS DE PRODUCCIÓN CON PYTHON**\n",
        "\n",
        "## **Taller 4 (8h): De Regresión Lineal a Machine Learning**\n",
        "\n",
        "### **El Dilema del Ajuste: Sobreajuste y Subajuste**\n",
        "\n",
        "**1. Entrenas un modelo y obtienes un 99% de exactitud sobre los datos de entrenamiento, pero solo un 75% sobre los datos de prueba. ¿Qué problema indica este resultado y por qué?**\n",
        "\n",
        "Cuando se entrena un modelo y se obtiene un 99% de exactitud en los datos de entrenamiento pero solo un 75% en los datos de prueba, lo que ocurre es un caso de sobreajuste; esto significa que el modelo se volvió demasiado complejo y terminó ajustándose no solo a los patrones reales de los datos, sino también al ruido y las particularidades irrelevantes del conjunto de entrenamiento, en consecuencia, aunque parece tener un desempeño casi perfecto con los datos que ya vio, falla al enfrentarse a datos nuevos porque no logra generalizar correctamente. La gran diferencia entre el rendimiento en entrenamiento y en validación es la señal más clara de este problema.\n",
        "\n",
        "**2. Si el error de tu modelo es muy alto tanto en el conjunto de entrenamiento como en el de validación, ¿cuál es el problema más probable? ¿Creerías que añadir más datos de entrenamiento solucionaría el problema?**\n",
        "\n",
        "Si el error del modelo resulta muy alto tanto en el conjunto de entrenamiento como en el de validación, el problema más probable es el subajuste; esto ocurre porque el modelo es demasiado simple y no tiene la capacidad suficiente para aprender los patrones subyacentes de los datos; en este escenario, el desempeño es pobre en ambos conjuntos, lo que indica que el modelo tiene un sesgo elevado, en este caso, añadir más datos de entrenamiento no solucionaría el problema, ya que el inconveniente no está en la cantidad de información disponible, sino en la falta de complejidad del modelo. La solución estaría en aumentar su capacidad, utilizar algoritmos más potentes o enriquecer las variables de entrada para capturar mejor la estructura del problema.\n",
        "\n",
        "### **El Dilema del Modelo y la Regularización Ridge y Lasso**\n",
        "\n",
        "**1. En un problema para predecir fallas en una máquina, tienes 100 variables provenientes de sensores, pero sospechas que solo unas pocas son realmente importantes. ¿Usarías Ridge o Lasso? Justifica tu respuesta.**\n",
        "\n",
        "En este problema la técnica más adecuada sería Lasso; esto se debe a que Lasso, mediante su penalización L1, tiene la capacidad de reducir algunos coeficientes exactamente a cero, lo que equivale a realizar una selección automática de variables, de esta manera, el modelo no solo se regulariza, sino que también se simplifica al quedarse únicamente con los predictores más relevantes, esto es especialmente útil en entornos industriales donde abundan las lecturas redundantes o ruidosas, ya que facilita la interpretabilidad del modelo y concentra la atención en las variables críticas para anticipar fallas.\n",
        "\n",
        "**2. Si entrenas un modelo Lasso y aumentas gradualmente el valor del hiperparámetro de penalización (λ), ¿qué efecto esperarías observar en los coeficientes del modelo?**\n",
        "\n",
        "En este caso el efecto esperado es que los coeficientes del modelo tiendan a disminuir progresivamente en magnitud y, en muchos casos, algunos de ellos se reduzcan a exactamente cero; en otras palabras, al incrementar la fuerza de la penalización, el modelo se vuelve más restrictivo y elimina las variables menos influyentes, manteniendo solo aquellas que tienen un impacto significativo en la predicción, esto refleja la naturaleza de Lasso como una herramienta de selección de características, lo que permite que el modelo final sea más compacto y manejable.\n",
        "\n",
        "**3. Al ejecutar el código de regularización 3D, ¿qué sucede con los coeficientes del modelo a medida que aumenta el valor de λ? ¿Qué interpretación le das a la forma diferente en que Ridge y Lasso aplican sus penalizaciones?**\n",
        "\n",
        "Al ejecutar un código de regularización en un entorno 3D y observar el comportamiento de los coeficientes del modelo a medida que aumenta el valor de λ, se nota que en ambos casos (Ridge y Lasso) los coeficientes disminuyen en magnitud, pero lo hacen de manera distinta; en Ridge, la penalización L2 reduce todos los coeficientes de forma proporcional, de modo que se vuelven más pequeños pero rara vez llegan a ser exactamente cero, esto da lugar a modelos más estables cuando muchas variables son relevantes; en cambio, en Lasso, la penalización L1 tiende a “forzar” algunos coeficientes a ser exactamente cero, gracias a la geometría de su restricción en forma de rombo, lo que permite seleccionar un subconjunto de variables relevantes. La interpretación de esta diferencia es que Ridge se centra en controlar la magnitud de todos los predictores, mientras que Lasso además actúa como un mecanismo de simplificación y selección de variables, lo que resulta especialmente valioso en problemas con un gran número de características redundantes.\n",
        "\n",
        "### **GridSearchCV: Encontrando la Mejor Configuración para tu Modelo**\n",
        "\n",
        "**1. Quieres optimizar un modelo Ridge y pruebas manualmente alpha=10, obteniendo un buen resultado. ¿Por qué sigue siendo metodológicamente superior usar GridSearchCV en lugar de quedarte con ese valor?**\n",
        "\n",
        "Aunque al probar manualmente un valor de alpha como 10 en un modelo Ridge se obtenga un buen resultado, utilizar GridSearchCV sigue siendo metodológicamente superior; esto es  debiaod a que GridSearchCV no depende de un único valor elegido de manera arbitraria, sino que explora de forma sistemática una rejilla de posibles valores y evalúa cada uno mediante validación cruzada, lo que asegura que el hiperparámetro seleccionado es realmente el que ofrece el mejor desempeño promedio en distintos subconjuntos de los datos de entrenamiento, reduciendo el riesgo de que el buen resultado con alpha=10 sea producto de la casualidad o de una división específica de los datos.\n",
        "\n",
        "**2. Además del modelo en sí (ej. Lasso()), ¿cuáles son los dos componentes principales que debes proporcionar a GridSearchCV para iniciar la búsqueda de hiperparámetros?**\n",
        "\n",
        "Los dos componentes principales que se deben proporcionar a GridSearchCV son:\n",
        "\n",
        "- La rejilla de parámetros (param_grid), que contiene los hiperparámetros y los valores a explorar, como diferentes niveles de alpha.\n",
        "\n",
        "- La estrategia de validación cruzada (cv), que define cómo se dividirán los datos en subconjuntos para evaluar de forma robusta cada configuración de hiperparámetros. Estos dos elementos, junto con el modelo, son la base para que GridSearchCV pueda automatizar el proceso de búsqueda y seleccionar la configuración que maximice el rendimiento.\n",
        "\n",
        "**3. Si GridSearchCV selecciona un alpha muy pequeño (cercano a cero) como el mejor parámetro para tu modelo, ¿qué te sugiere esto sobre el nivel de sobreajuste que tenía tu modelo original sin regularizar?**\n",
        "\n",
        "Esto sugiere que el modelo original sin regularizar no estaba presentando un sobreajuste significativo, en otras palabras, el modelo ya tenía una buena capacidad de generalización y no necesitaba una penalización fuerte para controlar la complejidad de sus coeficientes; la regularización, en este caso, actúa de manera mínima, confirmando que el riesgo de varianza elevada o de un ajuste excesivo al ruido era bajo, este hallazgo es importante, porque indica que el problema no estaba en la complejidad del modelo, sino posiblemente en otros factores como la calidad de los datos o la selección de variables.\n",
        "\n",
        "### **Construir un Árbol de Decisión: El Diagrama de Flujo Inteligente para la Optimización de Procesos**\n",
        "\n",
        "**1. En un árbol de decisión para optimizar la logística de un almacén, ¿qué podría representar un nodo hoja?**\n",
        "\n",
        "Un nodo hoja representaría la predicción final o decisión concreta después de haber evaluado una serie de condiciones. Por ejemplo, un nodo hoja podría indicar: “Asignar el pedido a la zona de picking rápido” o “Enviar el producto al área de almacenamiento en frío”. En otras palabras, cada hoja resume el camino lógico seguido a través de las variables del proceso y da una instrucción final clara y accionable que optimiza la operación logística.\n",
        "\n",
        "**2. Un ingeniero crea un árbol para predecir fallos en una máquina. El árbol es extremadamente profundo y tiene reglas muy específicas como \"Si la temperatura es 75.3°C y la vibración es 0.152 m/s² y el operador es Juan...\". ¿Qué problema de ajuste es este y por qué no sería fiable en la práctica diaria de la planta?**\n",
        "\n",
        "El problema que aparece es el sobreajuste (overfitting); en este caso, el árbol no está capturando patrones generales del proceso, sino memorias muy particulares de los datos históricos, incluyendo ruido y coincidencias irrelevantes, esto lo hace poco fiable para la práctica diaria de la planta, porque aunque pueda predecir perfectamente los datos pasados, fracasará al enfrentarse a nuevas condiciones de operación; en términos industriales, equivaldría a crear reglas demasiado específicas que no se sostienen en escenarios futuros y, por lo tanto, no aportan una base robusta para la mejora continua.\n",
        "\n",
        "### **Evaluando el Diagnóstico: La Matriz de Confusión y el F1-Score**\n",
        "\n",
        "**1. Al visualizar la \"importancia de las características\" de tu árbol, descubres que el \"proveedor de materia prima\" es la variable más importante. ¿Qué acción inmediata podrías tomar en la planta con esta información?**\n",
        "\n",
        "La acción inmediata en planta sería revisar a fondo los lotes provenientes de ese proveedor, esto puede incluir auditorías de calidad, inspecciones más estrictas en los materiales recibidos, o incluso establecer un plan de mejora con el proveedor en cuestión; de esta forma, la empresa puede atacar de raíz una de las causas más críticas de defectos, reduciendo variabilidad en el proceso y evitando que piezas defectuosas lleguen a producción.\n",
        "\n",
        "**2. Si tu árbol de decisión está clasificando perfectamente los datos históricos pero falla mucho con los datos de la última semana (sobreajuste), ¿qué parámetro de poda ajustarías primero para que generalice mejor?**\n",
        "\n",
        "Si el árbol de decisión clasifica con gran exactitud los datos históricos pero muestra un desempeño deficiente con los datos recientes, se trata de un caso de sobreajuste; en este escenario, el parámetro de poda más recomendable para ajustar en primera instancia es max_depth, que limita la profundidad del árbol, al reducir cuántas divisiones consecutivas puede realizar el modelo, se evita que genere reglas excesivamente específicas a los datos pasados y se favorece la identificación de patrones más generales; alternativamente, también podría evaluarse un ajuste en min_samples_leaf, para asegurar que cada nodo hoja se base en un número mínimo de observaciones y no en casos aislados.\n",
        "\n",
        "### **Auditando al Experto Digital: Interpretación y Ajuste Fino del Árbol**\n",
        "\n",
        "**1. Al visualizar la \"importancia de las características\" de tu árbol, descubres que el \"proveedor de materia prima\" es la variable más importante. ¿Qué acción inmediata podrías tomar en la planta con esta información?**\n",
        "\n",
        "Cuando al interpretar la importancia de las características en el árbol de decisión se identifica que el proveedor de materia prima es la variable más influyente en la predicción de fallos, la acción inmediata en planta debería orientarse a reforzar el control sobre los insumos recibidos, esto puede incluir una inspección más estricta a los lotes provenientes de dicho proveedor, la implementación de controles de calidad adicionales antes de ingresar el material a la línea de producción, o incluso reuniones con el proveedor para revisar estándares de entrega y procesos de aseguramiento de la calidad; de esta manera, se aprovecha el conocimiento generado por el modelo no solo para predecir, sino también para prevenir defectos mediante acciones correctivas en la cadena de suministro.\n",
        "\n",
        "**2. Si tu árbol de decisión está clasificando perfectamente los datos históricos pero falla mucho con los datos de la última semana (sobreajuste), ¿qué parámetro de poda ajustarías primero para que generalice mejor?**\n",
        "\n",
        "Si el árbol de decisión muestra un rendimiento perfecto en los datos históricos pero falla al enfrentarse con los más recientes, es un síntoma claro de sobreajuste; en este caso, el primer parámetro de poda a ajustar sería max_depth, que controla la profundidad máxima del árbol. Al limitar la cantidad de divisiones sucesivas, el modelo deja de generar reglas demasiado específicas y logra enfocarse en patrones más generales y representativos. Como alternativa complementaria, también se podría ajustar min_samples_leaf, asegurando que cada hoja contenga un número mínimo de observaciones y, por ende, que las reglas finales estén respaldadas por una base estadística más sólida. Así, se consigue un modelo más equilibrado, menos dependiente de los datos históricos y con mejor capacidad de generalización hacia nuevas situaciones de producción.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ypIr0V851oRd"
      }
    }
  ]
}